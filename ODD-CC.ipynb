{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"ODD-CC.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"acnZlmLtznNM"},"source":["#### CC Outils d'optimisation pour les sciences des données et de la décision\n","##### Version 1.3 - *C. W. Royer, janvier 2021.*\n"," \n","Liste des corrections postérieures au premier dépôt :\n","\n","V 1.1 - Correction de la question 2 - On demande de montrer que $L_{\\max} \\ge L/d$.\n","\n","V 1.2 - Correction dans la formule de $w^*$ en question 8 ($\\tfrac{\\lambda}{L}$), correction d'un *obj3* en *obj4* dans le dernier bloc d'affichage.\n","\n","V 1.3 - Correction dans l'équation (3) de la question 6, on cherche bien à minimiser la norme de $\\mathbf{w}-\\mathbf{y}$. Ajout de la signification du paramètre *nblocs* dans le code **rcd_lasso**."]},{"cell_type":"markdown","metadata":{"id":"R4kNEhjYznNX"},"source":["# Outils d'optimisation pour les sciences des données et de la décision - Contrôle continu\n","\n","# Master 2 ID 2020-2021\n","\n","## Date de rendu : 22 janvier 2021\n","\n","## Modalités de rendu\n","\n","- Ce notebook est à rendre **individuellement**. Votre rendu devra comporter vos nom et prénom(s). Les réponses aux différentes questions devront être incorporées au notebook, et la partie pratique devra pouvoir être exécutée par l'enseignant.\n","\n","- Votre version du notebook est à envoyer par mail à l'adresse clement.royer@dauphine.psl.eu. La date limite est fixée au **22 janvier 2021**."]},{"cell_type":"markdown","metadata":{"id":"3ziU22RYznNY"},"source":["## Ressources utiles\n","\n","- *La version la plus récente de ce notebook se trouve dans l'onglet* Fichiers *de l'équipe Teams associée au cours, ou sur [ce lien](https://www.lamsade.dauphine.fr/~croyer/ensdocs/ODD/ODD-CC.ipynb).*\n","\n","- *Le polycopié de cours est disponible [ici](https://www.lamsade.dauphine.fr/~croyer/ensdocs/ODD/PolyODD.pdf).*\n","\n","- *Ce notebook utilise LaTeX pour la partie mathématiques.* ***Le bloc ci-dessous devra être executé pour permettre d'éditer le notebook de la même manière qu'un document LaTeX (en permettant, notamment, aux équations d'être numérotées et référencées dans le même bloc).***"]},{"cell_type":"code","metadata":{"id":"nHQLv5QkznNY","outputId":"e452b7f1-cef7-4dca-adb3-34b10a94ba13"},"source":["%%javascript\n","MathJax.Hub.Config({\n","    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n","});"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["MathJax.Hub.Config({\n","    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n","});\n"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"2LDHF6IKznNa"},"source":["- Des macros LaTeX sont définies ci-dessous (mais n'apparaissent plus une fois le bloc exécuté). N'hésitez pas à incorporer vos propres macros !\n","\n","$$\n","\\newcommand{\\E}[1]{\\operatorname{\\mathbb{E}}\\left[#1\\right]}\n","\\DeclareMathOperator*{\\argmin}{\\text{argmin}}\n","\\def\\R{{\\mathbb{R}}}\n","\\def\\vz{{\\mathbf{z}}}\n","\\def\\vy{{\\mathbf{y}}}\n","\\def\\vx{{\\mathbf{x}}}\n","\\def\\vw{{\\mathbf{w}}}\n","\\def\\vv{{\\mathbf{v}}}\n","\\def\\vu{{\\mathbf{u}}}\n","\\def\\vq{{\\mathbf{q}}}\n","\\def\\ve{{\\mathbf{e}}}\n","\\def\\va{{\\mathbf{a}}}\n","\\def\\mX{{\\mathbf{X}}}\n","\\def\\mQ{{\\mathbf{Q}}}\n","\\def\\calC{{\\mathcal{C}}}\n","\\def\\setB{{\\mathcal{B}}}\n","\\newcommand{\\T}{\\mathrm{T}}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"2m9OEDEbznNb"},"source":["### Notations\n","\n","- Dans la suite, $d$ et $n$ désigneront toujours des entiers supérieurs ou égaux à 1.\n","- Pour tout vecteur $\\vw \\in \\R^d$, le $i$-ème coefficient de $\\vw$ sera noté $[\\vw]_i$."]},{"cell_type":"markdown","metadata":{"id":"4PBqP-yZznNb"},"source":["# Sujet du notebook : Algorithmes de descente par coordonnées\n","\n","Les méthodes de descente par coordonnées (ou *coordinate descent* en anglais) sont une classe d'algorithmes qui a connu un regain d'intérêt avec l'arrivée des problèmes de traitement de données massives, pour lesquels il peut être trop coûteux d'agir sur toutes les variables simultanément. Si ces approches sont parmi les plus anciennes, elles ont été revisitées au cours des deux dernières décennies et sont toujours un sujet de recherche actif.\n","\n","Le but de ce notebook est donc de servir d'introduction aux méthodes de descente par coordonnées, dans un contexte générique mais aussi adapté à l'optimisation *parcimonieuse* ou creuse, pour laquelle on cherche des solutions possédant le plus de coefficients nuls possible."]},{"cell_type":"markdown","metadata":{"id":"pSign78yznNc"},"source":["# Partie I - Bases des méthodes de descente par coordonnées"]},{"cell_type":"markdown","metadata":{"id":"1FPASRyxznNc"},"source":["Pour introduire le principe des méthodes de descente par coordonnées, on considère un problème générique de la forme \n","$$\n","    \\min_{\\vw \\in \\R^d} f(\\vw),\n","$$\n","où la fonction $f$ est supposée convexe et de classe $\\calC^1$.\n"," \n","Un algorithme de descente par coordonnées part d'un point $\\vw_0 \\in \\R^d$; à chaque itération $k$, la méthode choisit un indice $i_k \\in \\{1,\\dots,d\\}$, une taille de pas $\\alpha_k>0$, et effectue l'itération suivante :\n","\\begin{equation}\n","\\label{eq:dcit} \n","    \\vw_{k+1} := \\vw_k - \\alpha_k \\nabla_{i_k} f(\\vw_k) \\ve_{i_k},\n","\\end{equation}\n","où $\\nabla_{i_k}$ est une notation qui désigne la dérivée partielle par rapport à la $i_k$-ème coordonnée, càd \n","$\\nabla_{i_k} f(\\vw) = [\\nabla f(\\vw)]_{i_k}$ pour tout $\\vw \\in \\R^d$, et $\\ve_{i_k}$ désigne le $i_k$-ème vecteur de la base canonique de $\\R^d$.\n","\n","Le but intrinsèque d'une telle méthode est donc de ramener la résolution d'un problème en $d$ dimensions à celle d'une suite de problèmes en une dimension, en modifiant une coordonnée à la fois. Dans le cas général, il ne suffira pas de faire $d$ itérations pour obtenir la convergence, en revanche des propriétés telles que la séparabilité permettent de garantir la pertinence d'un tel procédé.\n","\n","## I - A) Propriétés élémentaires\n","\n","**Question 1** On peut réécrire l'itération \\eqref{eq:dcit} comme $\\vw_{k+1}:=\\vw_k+c_k \\ve_{i_k}$, où\n","\\begin{equation}\n","\\label{eq:dcitpb}\n","    c_k = \\argmin_{c \\in \\R}\\ q_k(c), \\quad \\mbox{avec} \\quad q_k(c):=\\nabla_{i_k} f(\\vw_k) (c-[\\vw_k]_{i_k}) \n","    + \\frac{1}{2\\alpha_k}(c-[\\vw_k]_{i_k})^2.\n","\\end{equation}\n","\n","**1-a)** Justifier que ce problème est fortement convexe, et donner la valeur explicite de $c_k$. \n","\n","**1-b)** À quel principe algorithmique vu en cours pouvez-vous relier la caractérisation \\eqref{eq:dcitpb} ?\n","\n","**Question 2** Une des hypothèses classiques d'analyse de ce genre de méthodes consiste à supposer que le gradient de $f$ est lipschitzien selon chaque coordonnée, c'est-à-dire qu'il existe $d$ réels positifs $L_1,\\dots,L_d$ tels que\n","$$\n","    \\forall i=1,\\dots,d,\\ \\forall (\\vw,\\vv) \\in (\\R^d)^2, \n","    \\quad \\left| \\nabla_i f(\\vw) - \\nabla_i f(\\vv) \\right| \n","    \\le L_i \\|\\vw-\\vv\\|.\n","$$\n","On peut alors définir la *constante de Lipschitz par coordonnées* $L_{\\max}:= \\max_{1 \\le i \\le d} L_i$.\n","Justifier que si le gradient $\\nabla f$ est $L$-lipschitzien, alors $L_{\\max} \\ge \\frac{L}{d}$.\n","\n","**Question 3** Par analogie avec les approches de type descente de gradient, pourquoi peut-on envisager le choix de taille de pas $\\alpha_k = \\frac{1}{L_{i_k}}$ à l'itération $k$ de l'algorithme ? Quel serait l'intérêt de choisir ce pas plutôt que $\\frac{1}{L}$, comme vu en cours, ou que $\\frac{1}{L_{\\max}}$ ?\n"," \n","\n","## *Réponses de la partie I - A)*\n","\n","*Votre réponse à la question 1*\n","\n","*1-a)*\n","\n","*1-b)*\n","\n","*Votre réponse à la question 2*\n","\n","*Votre réponse à la question 3*"]},{"cell_type":"markdown","metadata":{"id":"AZJklYR4znNd"},"source":["## I - B) Variantes aléatoires\n","\n","On considère maintenant des variantes de la descente par coordonnées basée sur une sélection aléatoire des coordonnées.\n","\n","Dans la suite de cette partie, on supposera que l'on a accès aux constantes de Lipschitz selon chaque coordonnée, notées $\\{L_i\\}_{i=1}^d$. On considère alors la variante de descente par coordonnée où un indice $i_k$ est tiré ***uniformément au hasard*** dans $\\{1,\\dots,d\\}$, puis le nouvel itéré est calculé comme précédemment via l'itération \n","$$\n","        \\vw_{k+1} := \\vw_k - \\frac{\\nabla_{i_k} f(\\vw_k)}{L_{i_k}} \\, \\ve_{i_k},\n","$$\n","où l'on choisit $\\frac{1}{L_{i_k}}$ comme taille de pas.\n","\n","\n","## Questions de la partie I - B)\n","\n","**Question 4** Dans cette question, on suppose que la fonction $f$ (en plus d'être convexe) possède une unique solution $\\vw^* \\in \\R^d$ et on note $f^* = f(\\vw^*)$. Pour tout $\\epsilon>0$, on peut alors montrer que l'algorithme de descente par coordonnées converge vers un itéré $\\vw_k$ tel que \n","$$\n","    \\E{\\vw_k} - f^* \\le \\epsilon \n","$$\n","en au plus\n","$$\n","    \\frac{2 d L_{\\max} \\|\\vw_0-\\vw^*\\|^2}{\\epsilon}\n","$$\n","itérations. Comparer ce résultat avec celui d'une descente de gradient sur le même problème en utilisant un pas $\\frac{1}{L}$, avec $L$ une constante de Lipschitz pour $\\nabla f$.\n","\n","\n","**Question 5** Il existe un algorithme qui combine la descente par coordonnées randomisée et l'accélération de \n","Nesterov. Partant de $\\mathbf{w}_0 \\in \\mathbb{R}^d$ et $\\mathbf{v}_0=\\mathbf{w}_0$, l'algorithme tire à chaque itération $k$ un indice $i_k$ uniformément au hasard entre $1$ et $d$, puis effectue la récursion :\n","\\begin{equation}\n","\\label{eq:nesterovcd}\n","    \\left\\{ \n","        \\begin{array}{lll}\n","            \\vu_k &:= &\\lambda_k \\vv_k + (1-\\lambda_k) \\vw_k \\\\\n","            \\vw_{k+1} &:= &\\vu_k - \\frac{1}{L_{i_k}} \\nabla_{i_k} f(\\vu_k) \\ve_{i_k} \\\\\n","            \\vv_{k+1} &:= &\\mu_k \\vv_k + (1-\\mu_k) \\vu_k - \\frac{\\gamma_k}{L_{i_k}}\\nabla_{i_k} f(\\vu) \\ve_{i_k},\n","        \\end{array}\n","    \\right.\n","\\end{equation}\n","où $\\{\\lambda_k,\\mu_k,\\gamma_k\\}$ sont des suites de réels déterminés en fonction de $d$ (voire de la constante de convexité forte si $f$ est fortement convexe).\n","\n","Cet algorithme obtient une meilleure complexité en $\\mathcal{O}(\\frac{1}{k^2})$. En observant l'itération \\eqref{eq:nesterovcd}, quel inconvénient pratique décelez-vous dans ces récursions ?\n","\n","**Question 6** Avec la stratégie \"randomisée\" décrite dans cette section, l'algorithme de descente par coordonnées rappelle l'algorithme du gradient stochastique. Il existe de fait une connection forte entre ces deux méthodes. Pour l'illustrer, on considère le problème aux moindres carrés linéaires\n","\\begin{equation}\n","\\label{eq:ermsg}\n","    \\min_{\\vv \\in \\R^d} \\frac{1}{2d} \\left\\|\\mQ \\vv -\\vy \\right\\|^2 = \\frac{1}{2d} \\sum_{i=1}^d (\\vq_i^\\T \\vv - [\\vy]_i)^2,\n","\\end{equation}\n","où $\\mQ = \\left[ \\begin{array}{c} \\vq_1^\\T \\\\ \\vdots \\\\ \\vq_d^\\T \\end{array} \\right]$ est une matrice orthogonale, c'est-à-dire inversible avec $\\mQ^\\T = \\mQ^{-1}$. Résoudre le problème \\eqref{eq:ermsg} est alors équivalent à résoudre le problème\n","\\begin{equation}\n","\\label{eq:ermcd}\n","    \\min_{\\vw \\in \\R^d} \\frac{1}{2} \\left\\|\\vw -\\vy \\right\\|^2 = \\frac{1}{2} \\sum_{i=1}^d ([\\vw]_i - [\\vy]_i)^2.\n","\\end{equation}\n","\n","Montrer qu'une itération du gradient stochastique appliqué à \\eqref{eq:ermsg} et une itération de la descente par coordonnées randomisée appliquée à \\eqref{eq:ermcd} avec la même taille de pas et le même tirage d'indice sont équivalentes, dans le sens où si $\\vv_k = \\mQ^\\T \\vw_k$, alors $\\vv_{k+1}=\\mQ^\\T \\vw_{k+1}$.\n","\n","## *Réponses de la partie I - B)*\n","\n","*Votre réponse à la question 4*\n","\n","*Votre réponse à la question 5*\n","\n","*Votre réponse à la question 6*\n"]},{"cell_type":"markdown","metadata":{"id":"b6nxd0egznNe"},"source":["# PARTIE II - Descente par coordonnées et optimisation parcimonieuse"]},{"cell_type":"markdown","metadata":{"id":"_A664abCznNf"},"source":["Dans cette partie, on étudie l'intérêt des méthodes de descente par coordonnées pour les problèmes de la forme\n","$$\n","    \\min_{\\vw \\in \\R^d} f(\\vw) + \\lambda \\Omega(\\vw),\n","$$\n","où $f$ sera une fonction de classe $\\calC^1$, $\\lambda>0$ et $\\Omega$ est un terme de régularisation. On supposera que $\\Omega$ possède une structure dite séparable, c'est-à-dire qu'il existe une fonction $h:[0,\\infty) \\rightarrow [0,\\infty)$ telle que\n","$$\n","    \\forall \\vw \\in \\R^d, \\quad \\Omega(\\vw) = \\sum_{i=1}^d h(\\left|[\\vw]_i \\right|).\n","$$\n","L'exemple classique d'une telle fonction est la fonction $h(t):=t$, qui conduit à une régularisation en norme $\\ell_1$ de la forme $\\Omega(\\vw):=\\|\\vw\\|_1$, avec $\\|\\vw\\|_1 := \\sum_{i=1}^d |[\\vw]_i|$. On se concentrera sur ce choix dans ce notebook pour montrer l'intérêt des méthodes de descente par coordonnées, mais on notera que les idées majeures présentées ci-dessous s'étendent à d'autres régularisations de la forme pré-citée.\n","\n","\n","## II - A) Un problème unidimensionnel\n","\n","On considère le problème\n","$$\n","    \\min_{w \\in \\mathbb{R}}\\ f_1(w), \\quad \\mbox{avec} \\quad f_1(w):= a (w-u) +  \\frac{L}{2} (w-u)^2 + \\lambda |w|,\n","$$\n","où $a,u \\in \\mathbb{R}$, $L>0$, et $\\lambda \\ge 0$.\n","\n","## Questions de la partie II - A)\n","\n","**Question 7** Montrer que ce problème est fortement convexe.\n","\n","**Question 8** On rappelle que la fonction $f_1$ est continue mais non dérivable partout. Comme vu en cours, son *sous-différentiel* est décrit par:\n","$$\n","    \\partial f_1(w) := \\left\\{\n","        \\begin{array}{ll}\n","            a + L(w-u) + \\lambda &\\mbox{si $w>0$}\\\\\n","            a + L(w-u) -\\lambda &\\mbox{si $w<0$} \\\\\n","            [a + L(w-u)-\\lambda,a + L(w-u)+\\lambda] &\\mbox{si $w=0$.}\n","        \\end{array}\n","    \\right.\n","$$\n","Ainsi, le minimum global de $\\min_{w \\in \\mathbb{R}} f_1(w)$, noté $w^*$, est caractérisé par la condition \n","$$ \n","    0 \\in \\partial f_1(w^*).\n","$$\n","\n","En utilisant ce sous-différentiel, montrer que $w^*$ est donné par\n","$$\n","    w^* = \\left\\{\n","        \\begin{array}{ll}\n","            u-\\tfrac{a}{L} - \\tfrac{\\lambda}{L} &\\mbox{si $u-\\tfrac{a}{L} > \\tfrac{\\lambda}{L}$} \\\\\n","            u-\\tfrac{a}{L} + \\tfrac{\\lambda}{L} &\\mbox{si $u-\\tfrac{a}{L} < -\\tfrac{\\lambda}{L}$} \\\\\n","            0 &\\mbox{si $u-\\tfrac{a}{L} \\in [-\\tfrac{\\lambda}{L},\\tfrac{\\lambda}{L}]$.}\n","        \\end{array}\n","    \\right.\n","$$\n","\n","## *Réponses de la partie II - A)*\n","\n","*Votre réponse à la question 7*\n","\n","*Votre réponse à la question 8*\n","\n","## II-B) Cas général et descente de coordonnées par blocs\n","\n","On considère maintenant un problème de type LASSO, aussi appelé problème de poursuite de base (de l'anglais *basis pursuit*) en traitement du signal. Pour un jeu de données formé par une matrice $\\mX \\in \\R^{n \\times d}$ et un vecteur $\\vy \\in \\R^d$, ce problème s'écrit\n","\\begin{equation}\n","\\label{eq:basispursuit}\n","    \\min_{\\vw \\in \\R^d} f(\\vw)+ \\lambda \\|\\vw_1\\|, \\quad \\mbox{avec} \\quad\n","    f(\\vw):= \\frac{1}{2 n} \\|\\vy - \\mX \\vw \\|^2.\n","\\end{equation}\n","Il s'agit donc d'un problème avec régularisation $\\ell_1$. Pour le résoudre, nous allons utiliser une approche par blocs de coordonnées, dans laquelle nous utiliserons un bloc $\\setB_k \\subset \\{1,\\dots,d\\}$ de coordonnées tiré uniformément au hasard à chaque itération. Le nouveau point sera ainsi déterminé en résolvant le sous-problème\n","\\begin{equation}\n","\\label{eq:coord}\n","    \\vw_{k+1} \\in \\argmin_{\\vw} f_{\\setB_k}(\\vw):= f(\\vw_k) + \\sum_{i \\in \\setB_k} \\left\\{ \n","    \\left([\\vw]_i-[\\vw_k]_i\\right) \\nabla_i f(\\vw_k)+ \\frac{L_i}{2} \\left([\\vw]_i-[\\vw_k]_i\\right)^2 \n","    + \\lambda \\left| [\\vw_i] \\right| \\right\\},\n","\\end{equation}\n","où $L_{i_k}$ représente la constante de Lipschitz correspondant à la $i_k$-ème coordonnée, que l'on suppose connue comme en partie I-B).\n","\n","## Questions de la partie II - B)\n","\n","\n","**Question 9**  On cherche à implémenter l'algorithme ci-dessus.\n","\n","**9-a)** En utilisant le résultat de la partie II-A), justifier que le nouvel itéré de l'algorithme de descente coordonnées par blocs est donné par\n","\\begin{equation}\n","\\label{eq:bcdup}\n","    \\forall i \\in \\{1,\\dots,d\\}, \\quad \n","    [\\vw_{k+1}]_i = \\left\\{\n","    \\begin{array}{ll}\n","        [\\vw_k]_i &\\mbox{si $i \\notin \\setB_k$} \\\\\n","        [\\vw_k]_i - \\tfrac{1}{L_i} \\nabla_i f(\\vw_k) - \\tfrac{\\lambda}{L_i} &\\mbox{si $i \\in \\setB_k$ et } \n","        [\\vw_k]_i - \\tfrac{1}{L_i} \\nabla_i f(\\vw_k) > \\tfrac{\\lambda}{L_i} \\\\\n","        [\\vw_k]_i - \\tfrac{1}{L_i} \\nabla_i f(\\vw_k) + \\tfrac{\\lambda}{L_i} &\\mbox{si $i \\in \\setB_k$ et } \n","        [\\vw_k]_i - \\tfrac{1}{L_i} \\nabla_i f(\\vw_k) < - \\tfrac{\\lambda}{L_i} \\\\\n","        0 &\\mbox{sinon.}\n","    \\end{array}\n","    \\right.\n","\\end{equation}\n","\n","**9-b)** Si on choisit tous les $L_i$ égaux, à quel algorithme vu en cours correspond la variante par blocs pour laquelle $|\\setB| = n$ ?\n","\n","**Question 10** Compléter le code-ci-dessous pour implémenter un algorithme de descente par coordonnées randomisé adapté au problème \\eqref{eq:basispursuit} en utilisant la formule \\eqref{eq:bcdup}. Tester ensuite plusieurs valeurs de blocs de coordonnées en utilisant le script fourni, et commenter vos résultats.\n","\n","\n","## *Réponses aux questions de la partie II - B)*\n","\n","*Votre réponse à la question 9*\n","\n","*Vos commentaires concernant la question 10*"]},{"cell_type":"code","metadata":{"id":"-3q_brCUznNg"},"source":["# Implémentation de l'algorithme de descente par coordonnées randomisé (RCD)\n","# À compléter pour répondre à la question 10\n","def rcd_lasso(w0,X,y,lbda,nblocs=1,nits=500): \n","    \"\"\"\n","        Code de la descente par coordonnées pour les problèmes de type LASSO de la forme\n","        \n","        Entrées :\n","            w0: Point initial\n","            X : Matrice du problème concerné\n","            y : Vecteur du problème concerné\n","            lbda : Coefficient de régularisation\n","            nblocs : Nombre de coordonnées considérées à chaque itération\n","            nits: Nombre maximum d'itérations à effectuer, utilisé comme critère d'arrêt\n","            \n","        Sorties :\n","            w_output : Dernier itéré calculé par l'algorithme\n","            objvals : Historique des valeurs de fonction (tableau Numpy de longueur au plus nits)\n","            nnzvals : Historique de parcimonie des itérés (tableau Numpy de longueur au plus nits)\n","    \"\"\"\n","    \n","    ############\n","    # Initialisation des historiques\n","    objvals = []\n","    nnzvals = []\n","    \n","    # Valeur initiale de l'itéré (le .copy() permet de ne pas modifier le point de départ) \n","    w = w0.copy()\n","\n","    # Initialisation de l'indice d'itération\n","    k=0    \n","    \n","    # Dimensions\n","    n,d = X.shape\n","    \n","    # Constantes de Lipschitz\n","    ell = norm(np.matmul(X.T,X),axis=0)\n","    \n","    # Calcul de l'objectif en le point initial+Ajout à l'historique\n","    obj = norm(X.dot(w) - y) ** 2 / (2. * n) + lbda * norm(w,1)\n","    objvals.append(obj)\n","    \n","    # Calcul du nombre de coefficients non nuls dans l'itéré\n","    nnzvals.append(np.count_nonzero(w))\n","    \n","    #### PARTIE A COMPLETER\n","    # Calcul du gradient de la partie lisse en le point initial\n","    g = # A compléter\n","    #### FIN PARTIE A COMPLETER\n","\n","    #########################\n","    # Début boucle principale\n","    while (k < nits):\n","        \n","        ### DEBUT PARTIE A COMPLETER\n","        # \n","        # Tirage aléatoire de la ou des coordonnées\n","        \n","        # Calcul du nouvel itéré par composantes (modifier le vecteur w existant)\n","                \n","        # Calcul du gradient de la partie lisse en le nouveau point\n","        g = # A completer\n","             \n","        ### FIN PARTIE A COMPLETER\n","\n","        # Calcul de l'objectif en le nouveau point ()\n","        obj = norm(X.dot(w) - y) ** 2 / (2. * n) + lbda * norm(w,1)\n","        objvals.append(obj)\n","        \n","        # Calcul du nombre de coefficients non nuls en le nouveau point\n","        nnzvals.append(np.count_nonzero(w))\n","        \n","        k += 1  \n","    # Fin boucle principale   \n","    #######################\n","    \n","    w_output = w.copy()\n","          \n","    return w_output, np.array(objvals), np.array(nnzvals)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mk2cvZdPznNh"},"source":["# Script de validation de l'implémentation de la question 10\n","\n","################# Imports préliminaires\n","%matplotlib inline\n","\n","import numpy as np # NumPy (calcul scientifique)\n","from scipy.linalg import norm # Norme euclidienne\n","import matplotlib.pyplot as plt # Affichage\n","\n","############### Etape 1 - Génération des données avec une vérité terrain parcimonieuse\n","#\n","# Les coefficients de X suivent une loi normale N(0,1/n)\n","# Le vecteur y est obtenu via y = X*w+eps, où\n","#    w est un vecteur creux (10% de coefficients non nuls)\n","from numpy.random import multivariate_normal, randn\n","\n","d = 200\n","n = 200\n","s = round(0.9*min(d,n))\n","X = multivariate_normal(np.zeros(d), (1/n)*np.identity(d), size=n)\n","idx = np.arange(d)\n","\n","# Coefficients du vrai modèle (\"vérité terrain/ground truth\")\n","wtrue = (-1)**idx * np.exp(-idx / 10.)\n","ip = np.random.permutation(d)\n","wtrue[ip[0:s]]=0\n","\n","Xw = X.dot(wtrue)\n","std = (0.01/n)*(norm(Xw)**2)\n","noise = std * randn(n)\n","y = Xw + noise\n","\n","w0 = np.ones(d)\n","lbda = 1/ (n**0.5)\n","\n","\n","################# Etape 2 - Tester plusieurs valeurs de tailles de bloc\n","\n","nb=1\n","nits=2000\n","w1,obj1,nnz1 = rcd_lasso(w0,X,y,lbda,nb,nits)\n","print('Nombre de bloc(s) :',nb)\n","print('Valeur objectif :',obj1[-1])\n","print('Pourcentage coefficients non nuls :',(d-nnz1[-1])*100/d)\n","nb=int(d/100)\n","w2,obj2,nnz2 = rcd_lasso(w0,X,y,lbda,nb,nits)\n","print('Nombre de bloc(s) :',nb)\n","print('Valeur objectif :',obj2[-1])\n","print('Pourcentage coefficients non nuls :',(d-nnz2[-1])*100/d)\n","nb=int(d/20)\n","w3,obj3,nnz3 = rcd_lasso(w0,X,y,lbda,nb,nits)\n","print('Nombre de bloc(s) :',nb)\n","print('Valeur objectif :',obj3[-1])\n","print('Pourcentage coefficients non nuls :',(d-nnz3[-1])*100/d)\n","nb=d\n","w4,obj4,nnz4 = rcd_lasso(w0,X,y,lbda,nb,nits)\n","print('Nombre de bloc(s) :',nb)\n","print('Valeur objectif :',obj4[-1])\n","print('Pourcentage coefficients non nuls :',(d-nnz4[-1])*100/d)\n","\n","################# Etape 3 - Affichage des résultats\n","\n","\n","\n","# En termes de fonction objectif (échelle logarithmique sur l'axe des ordonnées)\n","plt.figure(figsize=(7, 5))\n","plt.semilogy(obj1, label=\"nb=1\", color='indianred',lw=2)\n","plt.semilogy(obj2, label=\"nb=d/100\", color='brown', lw=2)\n","plt.semilogy(obj3, label=\"nb=d/20\", color='red', lw=2)\n","plt.semilogy(obj4, label=\"nb=d\", color='darkred', lw=2)\n","plt.title(\"Convergence des méthodes\", fontsize=16)\n","plt.xlabel(\"#Iterations\", fontsize=14)\n","plt.ylabel(\"Objectif (log)\", fontsize=14)\n","plt.legend()\n","\n","# Évolution du nombre de coordonnées nulles\n","plt.figure()\n","itnum = np.arange(nits+1)\n","print(itnum.shape)\n","print(nnz4.shape)\n","plt.scatter(itnum,nnz1,color='m',marker='o',label='nb=1')\n","plt.scatter(itnum,nnz2,color='indigo',marker='d',label='nb=d/100')\n","plt.scatter(itnum,nnz3,color='b',marker='*',label='nb=d/20')\n","plt.scatter(itnum,nnz4,color='dodgerblue',marker='x',label='nb=d')\n","plt.title(\"Parcimonie des itérés\", fontsize=16)\n","plt.xlabel(\"#Iterations\", fontsize=14)\n","plt.ylabel(\"Coefficients non nuls\", fontsize=14)\n","plt.legend()"],"execution_count":null,"outputs":[]}]}